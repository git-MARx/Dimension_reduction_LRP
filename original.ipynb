{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np, warnings\n",
    "warnings.filterwarnings(\"ignore\") #Ignore warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest and process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.139771</td>\n",
       "      <td>0.062774</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>0.083873</td>\n",
       "      <td>-0.119156</td>\n",
       "      <td>0.073305</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>0.043477</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.022711</td>\n",
       "      <td>-0.050504</td>\n",
       "      <td>-0.035691</td>\n",
       "      <td>-0.065434</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>-0.378560</td>\n",
       "      <td>0.038212</td>\n",
       "      <td>0.085770</td>\n",
       "      <td>0.182613</td>\n",
       "      <td>-0.055544</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.090407</td>\n",
       "      <td>0.021198</td>\n",
       "      <td>0.208712</td>\n",
       "      <td>0.102752</td>\n",
       "      <td>0.119315</td>\n",
       "      <td>0.041729</td>\n",
       "      <td>-0.021728</td>\n",
       "      <td>0.019603</td>\n",
       "      <td>-0.063853</td>\n",
       "      <td>-0.053756</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.085235</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>-0.013228</td>\n",
       "      <td>0.094063</td>\n",
       "      <td>-0.013592</td>\n",
       "      <td>-0.030719</td>\n",
       "      <td>-0.116062</td>\n",
       "      <td>-0.131674</td>\n",
       "      <td>-0.165448</td>\n",
       "      <td>-0.123053</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.088765</td>\n",
       "      <td>-0.026743</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>-0.043819</td>\n",
       "      <td>-0.005465</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>-0.055865</td>\n",
       "      <td>-0.071484</td>\n",
       "      <td>-0.159025</td>\n",
       "      <td>-0.111348</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0  0.093700  0.139771  0.062774  0.007698  0.083873 -0.119156  0.073305   \n",
       "1 -0.022711 -0.050504 -0.035691 -0.065434 -0.084316 -0.378560  0.038212   \n",
       "2 -0.090407  0.021198  0.208712  0.102752  0.119315  0.041729 -0.021728   \n",
       "3 -0.085235  0.009540 -0.013228  0.094063 -0.013592 -0.030719 -0.116062   \n",
       "4 -0.088765 -0.026743  0.002075 -0.043819 -0.005465  0.004306 -0.055865   \n",
       "\n",
       "       Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0  0.005510  0.027523  0.043477  ...    b'0'    b'0'    b'0'    b'0'    b'0'   \n",
       "1  0.085770  0.182613 -0.055544  ...    b'0'    b'0'    b'1'    b'1'    b'0'   \n",
       "2  0.019603 -0.063853 -0.053756  ...    b'0'    b'0'    b'0'    b'0'    b'0'   \n",
       "3 -0.131674 -0.165448 -0.123053  ...    b'0'    b'0'    b'0'    b'0'    b'0'   \n",
       "4 -0.071484 -0.159025 -0.111348  ...    b'0'    b'0'    b'0'    b'0'    b'0'   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0     b'0'     b'0'     b'0'     b'0'     b'0'  \n",
       "1     b'0'     b'0'     b'1'     b'1'     b'0'  \n",
       "2     b'0'     b'0'     b'1'     b'1'     b'0'  \n",
       "3     b'0'     b'0'     b'1'     b'1'     b'1'  \n",
       "4     b'0'     b'0'     b'0'     b'0'     b'0'  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain = pd.read_csv('data/yeast/yeast-train.csv')\n",
    "datatest = pd.read_csv('data/yeast/yeast-test.csv')\n",
    "\n",
    "datatrain.loc[datatrain['Class1'] ==\"b'0'\", 'Class1'] = 0\n",
    "datatrain.loc[datatrain['Class1'] ==\"b'1'\", 'Class1'] = 1\n",
    "datatest.loc[datatest['Class1'] ==\"b'0'\", 'Class1'] = 0\n",
    "datatest.loc[datatest['Class1'] ==\"b'1'\", 'Class1'] = 1\n",
    "\n",
    "dataset = pd.concat([datatrain, datatest], ignore_index=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNum=1\n",
    "y_class = 102 + classNum\n",
    "X = dataset.iloc[:,:103].values\n",
    "y = dataset.iloc[:,y_class:y_class+1].values\n",
    "# X_train = datatrain.iloc[:,:103].values\n",
    "# y_train = datatrain.iloc[:,y_class:y_class+1].values\n",
    "# X_test = datatest.iloc[:,:103].values\n",
    "# y_test = datatest.iloc[:,y_class:y_class+1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset['Class1'].value_counts()\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=np_utils.to_categorical(y, num_classes = 2)\n",
    "X_train,X_test, y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1933, 103)\n",
      "(484, 103)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [el/100 for el in range(0,100) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.96%\n"
     ]
    }
   ],
   "source": [
    "# mxa=0\n",
    "# for i in a:\n",
    "xgbclassifier = XGBClassifier(learning_rate=0.27)\n",
    "xgbclassifier.fit(X_train, y_train)\n",
    "y_pred = xgbclassifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "#     if accuracy>mxa:\n",
    "#         mxa=accuracy\n",
    "#         print(mxa,i+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved...\n",
      "\n",
      "Model Accuracy: 0.7995867768595041\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[307  23]\n",
      " [ 74  80]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.86       330\n",
      "           1       0.78      0.52      0.62       154\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       484\n",
      "   macro avg       0.79      0.72      0.74       484\n",
      "weighted avg       0.80      0.80      0.79       484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save model to file\n",
    "pickle.dump(xgbclassifier, open(\"models/xgbModelplain.pickle.dat\", \"wb\"))\n",
    "print(\"Model Saved...\\n\")\n",
    "\n",
    "score = xgbclassifier.score(X_test,y_test)\n",
    "print(\"Model Accuracy: \" + str(score) + \"\\n\")\n",
    "predY = xgbclassifier.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, predY))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, predY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[311  19]\n",
      " [ 85  69]]\n",
      "Accuracy: 78.51%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svmclassifier = SVC(kernel='linear' ,random_state=0)\n",
    "svmclassifier.fit(X_train,y_train)\n",
    "y_pred_svm = svmclassifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred_svm)\n",
    "print(cm)\n",
    "accuracy=accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved...\n",
      "\n",
      "Model Accuracy: 0.7851239669421488\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[311  19]\n",
      " [ 85  69]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86       330\n",
      "           1       0.78      0.45      0.57       154\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       484\n",
      "   macro avg       0.78      0.70      0.71       484\n",
      "weighted avg       0.78      0.79      0.77       484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save model to file\n",
    "pickle.dump(svmclassifier, open(\"models/svmModelplain.pickle.dat\", \"wb\"))\n",
    "print(\"Model Saved...\\n\")\n",
    "\n",
    "score = svmclassifier.score(X_test,y_test)\n",
    "print(\"Model Accuracy: \" + str(score) + \"\\n\")\n",
    "y_pred_svm = svmclassifier.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.34%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# mxa = 0\n",
    "# for i in range(1,50):\n",
    "rfclassifier = RandomForestClassifier(n_estimators=45,criterion='entropy',random_state=0)\n",
    "rfclassifier.fit(X_train,y_train)\n",
    "y_pred_randForest = rfclassifier.predict(X_test)\n",
    "accuracy=accuracy_score(y_test, y_pred_randForest)\n",
    "#     if accuracy>mxa:\n",
    "#         mxa=accuracy\n",
    "#         print(accuracy,(i*5))\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved...\n",
      "\n",
      "Model Accuracy: 0.7933884297520661\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[320  10]\n",
      " [ 90  64]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.97      0.86       330\n",
      "           1       0.86      0.42      0.56       154\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       484\n",
      "   macro avg       0.82      0.69      0.71       484\n",
      "weighted avg       0.81      0.79      0.77       484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save model to file\n",
    "pickle.dump(rfclassifier, open(\"models/rfModelplain.pickle.dat\", \"wb\"))\n",
    "print(\"Model Saved...\\n\")\n",
    "\n",
    "score = rfclassifier.score(X_test,y_test)\n",
    "print(\"Model Accuracy: \" + str(score) + \"\\n\")\n",
    "y_pred_randForest = rfclassifier.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_randForest))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_randForest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Packages for training model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_json\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# Packages for explanation\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdOptimizer = 'adam'\n",
    "lossFun='categorical_crossentropy'\n",
    "finalLayerActivation = 'softmax'\n",
    "batchSize=25\n",
    "numEpochs = 500\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Att1', 'Att2', 'Att3', 'Att4', 'Att5', 'Att6', 'Att7', 'Att8', 'Att9', 'Att10', 'Att11', 'Att12', 'Att13', 'Att14', 'Att15', 'Att16', 'Att17', 'Att18', 'Att19', 'Att20', 'Att21', 'Att22', 'Att23', 'Att24', 'Att25', 'Att26', 'Att27', 'Att28', 'Att29', 'Att30', 'Att31', 'Att32', 'Att33', 'Att34', 'Att35', 'Att36', 'Att37', 'Att38', 'Att39', 'Att40', 'Att41', 'Att42', 'Att43', 'Att44', 'Att45', 'Att46', 'Att47', 'Att48', 'Att49', 'Att50', 'Att51', 'Att52', 'Att53', 'Att54', 'Att55', 'Att56', 'Att57', 'Att58', 'Att59', 'Att60', 'Att61', 'Att62', 'Att63', 'Att64', 'Att65', 'Att66', 'Att67', 'Att68', 'Att69', 'Att70', 'Att71', 'Att72', 'Att73', 'Att74', 'Att75', 'Att76', 'Att77', 'Att78', 'Att79', 'Att80', 'Att81', 'Att82', 'Att83', 'Att84', 'Att85', 'Att86', 'Att87', 'Att88', 'Att89', 'Att90', 'Att91', 'Att92', 'Att93', 'Att94', 'Att95', 'Att96', 'Att97', 'Att98', 'Att99', 'Att100', 'Att101', 'Att102', 'Att103', 'Class1', 'Class2', 'Class3', 'Class4', 'Class5', 'Class6', 'Class7', 'Class8', 'Class9', 'Class10', 'Class11', 'Class12', 'Class13', 'Class14']"
     ]
    }
   ],
   "source": [
    "print(list(dataset.columns),end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rahul/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/rahul/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 103)               412       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                3328      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 6,174\n",
      "Trainable params: 5,840\n",
      "Non-trainable params: 334\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define training data\n",
    "features = ['Att1', 'Att2', 'Att3', 'Att4', 'Att5', 'Att6', 'Att7', 'Att8', 'Att9', \n",
    "            'Att10', 'Att11', 'Att12', 'Att13', 'Att14', 'Att15', 'Att16', 'Att17', \n",
    "            'Att18', 'Att19', 'Att20', 'Att21', 'Att22', 'Att23', 'Att24', 'Att25', \n",
    "            'Att26', 'Att27', 'Att28', 'Att29', 'Att30', 'Att31', 'Att32', 'Att33', \n",
    "            'Att34', 'Att35', 'Att36', 'Att37', 'Att38', 'Att39', 'Att40', 'Att41', \n",
    "            'Att42', 'Att43', 'Att44', 'Att45', 'Att46', 'Att47', 'Att48', 'Att49', \n",
    "            'Att50', 'Att51', 'Att52', 'Att53', 'Att54', 'Att55', 'Att56', 'Att57', \n",
    "            'Att58', 'Att59', 'Att60', 'Att61', 'Att62', 'Att63', 'Att64', 'Att65', \n",
    "            'Att66', 'Att67', 'Att68', 'Att69', 'Att70', 'Att71', 'Att72', 'Att73', \n",
    "            'Att74', 'Att75', 'Att76', 'Att77', 'Att78', 'Att79', 'Att80', 'Att81', \n",
    "            'Att82', 'Att83', 'Att84', 'Att85', 'Att86', 'Att87', 'Att88', 'Att89', \n",
    "            'Att90', 'Att91', 'Att92', 'Att93', 'Att94', 'Att95', 'Att96', 'Att97', \n",
    "            'Att98', 'Att99', 'Att100', 'Att101', 'Att102', 'Att103']\n",
    "x_train = dataset[features]\n",
    "inputDim = len(features)\n",
    "trainX = x_train\n",
    "\n",
    "# y_train = dataset['Class1']\n",
    "# trainY = np_utils.to_categorical(y_train, num_classes = nb_classes)\n",
    "trainY = np_utils.to_categorical(y_train, num_classes = nb_classes)\n",
    "testY = np_utils.to_categorical(y_test, num_classes = nb_classes)\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(inputDim,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(nb_classes, activation=finalLayerActivation))\n",
    "model.compile(loss=lossFun, optimizer=sgdOptimizer, metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1933, 103),\n",
       " (484, 103),\n",
       " (1933, 1),\n",
       " (484, 1),\n",
       " (1933, 2),\n",
       " (484, 2),\n",
       " (2417, 103))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape,trainY.shape,testY.shape,trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rahul/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1933 samples, validate on 484 samples\n",
      "Epoch 1/500\n",
      "1933/1933 [==============================] - 16s 8ms/step - loss: 0.8966 - acc: 0.5184 - val_loss: 0.6416 - val_acc: 0.6756\n",
      "Epoch 2/500\n",
      "1933/1933 [==============================] - 1s 259us/step - loss: 0.7394 - acc: 0.6048 - val_loss: 0.6263 - val_acc: 0.6818\n",
      "Epoch 3/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.6837 - acc: 0.6384 - val_loss: 0.6136 - val_acc: 0.6818\n",
      "Epoch 4/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.6782 - acc: 0.6601 - val_loss: 0.6078 - val_acc: 0.6818\n",
      "Epoch 5/500\n",
      "1933/1933 [==============================] - 1s 287us/step - loss: 0.6653 - acc: 0.6580 - val_loss: 0.6018 - val_acc: 0.6818\n",
      "Epoch 6/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.6379 - acc: 0.6632 - val_loss: 0.5984 - val_acc: 0.6839\n",
      "Epoch 7/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.6372 - acc: 0.6746 - val_loss: 0.5930 - val_acc: 0.6942\n",
      "Epoch 8/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.6222 - acc: 0.6803 - val_loss: 0.5880 - val_acc: 0.6942\n",
      "Epoch 9/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.6106 - acc: 0.6865 - val_loss: 0.5810 - val_acc: 0.6963\n",
      "Epoch 10/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.6047 - acc: 0.7056 - val_loss: 0.5722 - val_acc: 0.7066\n",
      "Epoch 11/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.5948 - acc: 0.7077 - val_loss: 0.5644 - val_acc: 0.7149\n",
      "Epoch 12/500\n",
      "1933/1933 [==============================] - 0s 259us/step - loss: 0.5890 - acc: 0.7015 - val_loss: 0.5598 - val_acc: 0.7149\n",
      "Epoch 13/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.5831 - acc: 0.7124 - val_loss: 0.5533 - val_acc: 0.7252\n",
      "Epoch 14/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.5832 - acc: 0.7160 - val_loss: 0.5444 - val_acc: 0.7417\n",
      "Epoch 15/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.5709 - acc: 0.7191 - val_loss: 0.5406 - val_acc: 0.7376\n",
      "Epoch 16/500\n",
      "1933/1933 [==============================] - 1s 259us/step - loss: 0.5670 - acc: 0.7217 - val_loss: 0.5390 - val_acc: 0.7273\n",
      "Epoch 17/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.5522 - acc: 0.7325 - val_loss: 0.5274 - val_acc: 0.7479\n",
      "Epoch 18/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.5531 - acc: 0.7346 - val_loss: 0.5256 - val_acc: 0.7541\n",
      "Epoch 19/500\n",
      "1933/1933 [==============================] - 1s 259us/step - loss: 0.5448 - acc: 0.7403 - val_loss: 0.5201 - val_acc: 0.7645\n",
      "Epoch 20/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.5470 - acc: 0.7372 - val_loss: 0.5224 - val_acc: 0.7583\n",
      "Epoch 21/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.5384 - acc: 0.7387 - val_loss: 0.5193 - val_acc: 0.7624\n",
      "Epoch 22/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.5354 - acc: 0.7419 - val_loss: 0.5176 - val_acc: 0.7645\n",
      "Epoch 23/500\n",
      "1933/1933 [==============================] - 1s 259us/step - loss: 0.5295 - acc: 0.7450 - val_loss: 0.5155 - val_acc: 0.7624\n",
      "Epoch 24/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.5301 - acc: 0.7429 - val_loss: 0.5120 - val_acc: 0.7624\n",
      "Epoch 25/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.5347 - acc: 0.7579 - val_loss: 0.5115 - val_acc: 0.7624\n",
      "Epoch 26/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.5277 - acc: 0.7522 - val_loss: 0.5120 - val_acc: 0.7624\n",
      "Epoch 27/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.5139 - acc: 0.7656 - val_loss: 0.5069 - val_acc: 0.7769\n",
      "Epoch 28/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.5204 - acc: 0.7522 - val_loss: 0.5035 - val_acc: 0.7810\n",
      "Epoch 29/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.5162 - acc: 0.7486 - val_loss: 0.4995 - val_acc: 0.7913\n",
      "Epoch 30/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.5217 - acc: 0.7470 - val_loss: 0.5020 - val_acc: 0.7789\n",
      "Epoch 31/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.5165 - acc: 0.7558 - val_loss: 0.5012 - val_acc: 0.7769\n",
      "Epoch 32/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.5043 - acc: 0.7625 - val_loss: 0.4993 - val_acc: 0.7831\n",
      "Epoch 33/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.5055 - acc: 0.7724 - val_loss: 0.4974 - val_acc: 0.7831\n",
      "Epoch 34/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.5135 - acc: 0.7667 - val_loss: 0.4957 - val_acc: 0.7831\n",
      "Epoch 35/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.5053 - acc: 0.7672 - val_loss: 0.4952 - val_acc: 0.7810\n",
      "Epoch 36/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.4950 - acc: 0.7641 - val_loss: 0.4957 - val_acc: 0.7851\n",
      "Epoch 37/500\n",
      "1933/1933 [==============================] - 1s 278us/step - loss: 0.4876 - acc: 0.7869 - val_loss: 0.4946 - val_acc: 0.7851\n",
      "Epoch 38/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.5065 - acc: 0.7734 - val_loss: 0.4924 - val_acc: 0.7913\n",
      "Epoch 39/500\n",
      "1933/1933 [==============================] - 1s 267us/step - loss: 0.4919 - acc: 0.7724 - val_loss: 0.4980 - val_acc: 0.7893\n",
      "Epoch 40/500\n",
      "1933/1933 [==============================] - 1s 259us/step - loss: 0.4905 - acc: 0.7781 - val_loss: 0.4957 - val_acc: 0.7913\n",
      "Epoch 41/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.4932 - acc: 0.7750 - val_loss: 0.4944 - val_acc: 0.7934\n",
      "Epoch 42/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4873 - acc: 0.7765 - val_loss: 0.4941 - val_acc: 0.7913\n",
      "Epoch 43/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.4812 - acc: 0.7770 - val_loss: 0.4927 - val_acc: 0.7996\n",
      "Epoch 44/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.4803 - acc: 0.7775 - val_loss: 0.4961 - val_acc: 0.7893\n",
      "Epoch 45/500\n",
      "1933/1933 [==============================] - 1s 267us/step - loss: 0.4809 - acc: 0.7796 - val_loss: 0.4930 - val_acc: 0.7934\n",
      "Epoch 46/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.4897 - acc: 0.7724 - val_loss: 0.4950 - val_acc: 0.7893\n",
      "Epoch 47/500\n",
      "1933/1933 [==============================] - 1s 266us/step - loss: 0.4875 - acc: 0.7832 - val_loss: 0.4989 - val_acc: 0.7872\n",
      "Epoch 48/500\n",
      "1933/1933 [==============================] - 1s 269us/step - loss: 0.4844 - acc: 0.7874 - val_loss: 0.4974 - val_acc: 0.7913\n",
      "Epoch 49/500\n",
      "1933/1933 [==============================] - 1s 267us/step - loss: 0.4771 - acc: 0.7822 - val_loss: 0.4974 - val_acc: 0.7913\n",
      "Epoch 50/500\n",
      "1933/1933 [==============================] - 1s 269us/step - loss: 0.4841 - acc: 0.7838 - val_loss: 0.5022 - val_acc: 0.7934\n",
      "Epoch 51/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.4723 - acc: 0.7724 - val_loss: 0.5005 - val_acc: 0.7955\n",
      "Epoch 52/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.4839 - acc: 0.7729 - val_loss: 0.5016 - val_acc: 0.7955\n",
      "Epoch 53/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.4628 - acc: 0.7838 - val_loss: 0.4950 - val_acc: 0.7913\n",
      "Epoch 54/500\n",
      "1933/1933 [==============================] - 0s 259us/step - loss: 0.4781 - acc: 0.7801 - val_loss: 0.4978 - val_acc: 0.7955\n",
      "Epoch 55/500\n",
      "1933/1933 [==============================] - 1s 271us/step - loss: 0.4837 - acc: 0.7817 - val_loss: 0.4933 - val_acc: 0.7955\n",
      "Epoch 56/500\n",
      "1933/1933 [==============================] - 0s 259us/step - loss: 0.4608 - acc: 0.8096 - val_loss: 0.4911 - val_acc: 0.7996\n",
      "Epoch 57/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.4687 - acc: 0.8003 - val_loss: 0.4903 - val_acc: 0.7913\n",
      "Epoch 58/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4576 - acc: 0.7931 - val_loss: 0.4953 - val_acc: 0.7913\n",
      "Epoch 59/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.4754 - acc: 0.7889 - val_loss: 0.4940 - val_acc: 0.7893\n",
      "Epoch 60/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.4620 - acc: 0.7900 - val_loss: 0.4943 - val_acc: 0.7872\n",
      "Epoch 61/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.4569 - acc: 0.7941 - val_loss: 0.4940 - val_acc: 0.7872\n",
      "Epoch 62/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4614 - acc: 0.7884 - val_loss: 0.4903 - val_acc: 0.7913\n",
      "Epoch 63/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.4545 - acc: 0.7936 - val_loss: 0.4935 - val_acc: 0.7975\n",
      "Epoch 64/500\n",
      "1933/1933 [==============================] - 0s 254us/step - loss: 0.4667 - acc: 0.7853 - val_loss: 0.4933 - val_acc: 0.7934\n",
      "Epoch 65/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.4579 - acc: 0.8070 - val_loss: 0.4938 - val_acc: 0.7934\n",
      "Epoch 66/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.4558 - acc: 0.7853 - val_loss: 0.4955 - val_acc: 0.7893\n",
      "Epoch 67/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4595 - acc: 0.7884 - val_loss: 0.4948 - val_acc: 0.7934\n",
      "Epoch 68/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.4487 - acc: 0.8008 - val_loss: 0.4936 - val_acc: 0.7913\n",
      "Epoch 69/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.4584 - acc: 0.7894 - val_loss: 0.5027 - val_acc: 0.7872\n",
      "Epoch 70/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.4564 - acc: 0.7894 - val_loss: 0.4972 - val_acc: 0.7934\n",
      "Epoch 71/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.4389 - acc: 0.7972 - val_loss: 0.5038 - val_acc: 0.7913\n",
      "Epoch 72/500\n",
      "1933/1933 [==============================] - 0s 254us/step - loss: 0.4443 - acc: 0.8081 - val_loss: 0.5064 - val_acc: 0.7831\n",
      "Epoch 73/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.4464 - acc: 0.8050 - val_loss: 0.4954 - val_acc: 0.7872\n",
      "Epoch 74/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.4496 - acc: 0.8065 - val_loss: 0.4970 - val_acc: 0.7913\n",
      "Epoch 75/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.4350 - acc: 0.8050 - val_loss: 0.4986 - val_acc: 0.7872\n",
      "Epoch 76/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4535 - acc: 0.8055 - val_loss: 0.4992 - val_acc: 0.7913\n",
      "Epoch 77/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.4487 - acc: 0.7915 - val_loss: 0.4934 - val_acc: 0.7913\n",
      "Epoch 78/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4507 - acc: 0.7998 - val_loss: 0.4895 - val_acc: 0.7831\n",
      "Epoch 79/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.4508 - acc: 0.8050 - val_loss: 0.5017 - val_acc: 0.7934\n",
      "Epoch 80/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.4364 - acc: 0.8184 - val_loss: 0.4971 - val_acc: 0.7872\n",
      "Epoch 81/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.4382 - acc: 0.8101 - val_loss: 0.4940 - val_acc: 0.7851\n",
      "Epoch 82/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.4431 - acc: 0.8019 - val_loss: 0.4952 - val_acc: 0.7851\n",
      "Epoch 83/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4213 - acc: 0.8184 - val_loss: 0.4986 - val_acc: 0.7789\n",
      "Epoch 84/500\n",
      "1933/1933 [==============================] - 1s 285us/step - loss: 0.4325 - acc: 0.8096 - val_loss: 0.4919 - val_acc: 0.7934\n",
      "Epoch 85/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.4419 - acc: 0.8019 - val_loss: 0.4916 - val_acc: 0.7872\n",
      "Epoch 86/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.4378 - acc: 0.8050 - val_loss: 0.4941 - val_acc: 0.7851\n",
      "Epoch 87/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.4385 - acc: 0.8070 - val_loss: 0.4909 - val_acc: 0.7934\n",
      "Epoch 88/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.4429 - acc: 0.7951 - val_loss: 0.4938 - val_acc: 0.7851\n",
      "Epoch 89/500\n",
      "1933/1933 [==============================] - 0s 243us/step - loss: 0.4349 - acc: 0.8050 - val_loss: 0.4949 - val_acc: 0.7851\n",
      "Epoch 90/500\n",
      "1933/1933 [==============================] - 0s 244us/step - loss: 0.4434 - acc: 0.8112 - val_loss: 0.4968 - val_acc: 0.7872\n",
      "Epoch 91/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.4265 - acc: 0.8132 - val_loss: 0.5159 - val_acc: 0.7851\n",
      "Epoch 92/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.4458 - acc: 0.8008 - val_loss: 0.4979 - val_acc: 0.7851\n",
      "Epoch 93/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4277 - acc: 0.8138 - val_loss: 0.5021 - val_acc: 0.7913\n",
      "Epoch 94/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.4174 - acc: 0.8138 - val_loss: 0.5000 - val_acc: 0.7789\n",
      "Epoch 95/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4216 - acc: 0.8174 - val_loss: 0.5031 - val_acc: 0.7893\n",
      "Epoch 96/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.4353 - acc: 0.8138 - val_loss: 0.5040 - val_acc: 0.7872\n",
      "Epoch 97/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4347 - acc: 0.8127 - val_loss: 0.5043 - val_acc: 0.7872\n",
      "Epoch 98/500\n",
      "1933/1933 [==============================] - 0s 254us/step - loss: 0.4240 - acc: 0.8189 - val_loss: 0.5077 - val_acc: 0.7810\n",
      "Epoch 99/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.4175 - acc: 0.8076 - val_loss: 0.5173 - val_acc: 0.7810\n",
      "Epoch 100/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.4224 - acc: 0.8107 - val_loss: 0.5216 - val_acc: 0.7769\n",
      "Epoch 101/500\n",
      "1933/1933 [==============================] - 1s 272us/step - loss: 0.4093 - acc: 0.8282 - val_loss: 0.5174 - val_acc: 0.7851\n",
      "Epoch 102/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.4038 - acc: 0.8272 - val_loss: 0.5189 - val_acc: 0.7872\n",
      "Epoch 103/500\n",
      "1933/1933 [==============================] - 0s 254us/step - loss: 0.4096 - acc: 0.8158 - val_loss: 0.5169 - val_acc: 0.7789\n",
      "Epoch 104/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.4065 - acc: 0.8350 - val_loss: 0.5146 - val_acc: 0.7831\n",
      "Epoch 105/500\n",
      "1933/1933 [==============================] - 0s 244us/step - loss: 0.3957 - acc: 0.8231 - val_loss: 0.5334 - val_acc: 0.7831\n",
      "Epoch 106/500\n",
      "1933/1933 [==============================] - 1s 274us/step - loss: 0.4134 - acc: 0.8288 - val_loss: 0.5249 - val_acc: 0.7851\n",
      "Epoch 107/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.4173 - acc: 0.8246 - val_loss: 0.5260 - val_acc: 0.7831\n",
      "Epoch 108/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.4167 - acc: 0.8195 - val_loss: 0.5117 - val_acc: 0.7810\n",
      "Epoch 109/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.4109 - acc: 0.8246 - val_loss: 0.5083 - val_acc: 0.7769\n",
      "Epoch 110/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.4027 - acc: 0.8189 - val_loss: 0.5110 - val_acc: 0.7789\n",
      "Epoch 111/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.4155 - acc: 0.8226 - val_loss: 0.5103 - val_acc: 0.7789\n",
      "Epoch 112/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.4193 - acc: 0.8138 - val_loss: 0.5078 - val_acc: 0.7789\n",
      "Epoch 113/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.4042 - acc: 0.8257 - val_loss: 0.5163 - val_acc: 0.7748\n",
      "Epoch 114/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3987 - acc: 0.8236 - val_loss: 0.5291 - val_acc: 0.7831\n",
      "Epoch 115/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.4218 - acc: 0.8169 - val_loss: 0.5197 - val_acc: 0.7810\n",
      "Epoch 116/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.4095 - acc: 0.8241 - val_loss: 0.5195 - val_acc: 0.7707\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.4011 - acc: 0.8282 - val_loss: 0.5231 - val_acc: 0.7769\n",
      "Epoch 118/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.4151 - acc: 0.8200 - val_loss: 0.5231 - val_acc: 0.7789\n",
      "Epoch 119/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.4280 - acc: 0.8117 - val_loss: 0.5109 - val_acc: 0.7748\n",
      "Epoch 120/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.4098 - acc: 0.8220 - val_loss: 0.5214 - val_acc: 0.7748\n",
      "Epoch 121/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.4294 - acc: 0.8215 - val_loss: 0.5222 - val_acc: 0.7769\n",
      "Epoch 122/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.4035 - acc: 0.8215 - val_loss: 0.5339 - val_acc: 0.7789\n",
      "Epoch 123/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.4086 - acc: 0.8251 - val_loss: 0.5249 - val_acc: 0.7665\n",
      "Epoch 124/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3780 - acc: 0.8381 - val_loss: 0.5435 - val_acc: 0.7686\n",
      "Epoch 125/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.4082 - acc: 0.8272 - val_loss: 0.5366 - val_acc: 0.7665\n",
      "Epoch 126/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3976 - acc: 0.8226 - val_loss: 0.5439 - val_acc: 0.7748\n",
      "Epoch 127/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.4193 - acc: 0.8205 - val_loss: 0.5255 - val_acc: 0.7707\n",
      "Epoch 128/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3953 - acc: 0.8298 - val_loss: 0.5350 - val_acc: 0.7707\n",
      "Epoch 129/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3985 - acc: 0.8241 - val_loss: 0.5242 - val_acc: 0.7748\n",
      "Epoch 130/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3975 - acc: 0.8334 - val_loss: 0.5290 - val_acc: 0.7769\n",
      "Epoch 131/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.4305 - acc: 0.8143 - val_loss: 0.5300 - val_acc: 0.7748\n",
      "Epoch 132/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.4247 - acc: 0.8246 - val_loss: 0.5194 - val_acc: 0.7727\n",
      "Epoch 133/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.4106 - acc: 0.8241 - val_loss: 0.5166 - val_acc: 0.7603\n",
      "Epoch 134/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3917 - acc: 0.8324 - val_loss: 0.5401 - val_acc: 0.7583\n",
      "Epoch 135/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.4065 - acc: 0.8360 - val_loss: 0.5255 - val_acc: 0.7603\n",
      "Epoch 136/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.4138 - acc: 0.8200 - val_loss: 0.5400 - val_acc: 0.7645\n",
      "Epoch 137/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3917 - acc: 0.8334 - val_loss: 0.5443 - val_acc: 0.7707\n",
      "Epoch 138/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3973 - acc: 0.8391 - val_loss: 0.5419 - val_acc: 0.7727\n",
      "Epoch 139/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.4067 - acc: 0.8267 - val_loss: 0.5282 - val_acc: 0.7686\n",
      "Epoch 140/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3895 - acc: 0.8272 - val_loss: 0.5340 - val_acc: 0.7748\n",
      "Epoch 141/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3955 - acc: 0.8293 - val_loss: 0.5314 - val_acc: 0.7624\n",
      "Epoch 142/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.4068 - acc: 0.8329 - val_loss: 0.5234 - val_acc: 0.7769\n",
      "Epoch 143/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3902 - acc: 0.8251 - val_loss: 0.5296 - val_acc: 0.7810\n",
      "Epoch 144/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.4196 - acc: 0.8251 - val_loss: 0.5203 - val_acc: 0.7769\n",
      "Epoch 145/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.4018 - acc: 0.8339 - val_loss: 0.5277 - val_acc: 0.7810\n",
      "Epoch 146/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3945 - acc: 0.8345 - val_loss: 0.5297 - val_acc: 0.7727\n",
      "Epoch 147/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3967 - acc: 0.8376 - val_loss: 0.5293 - val_acc: 0.7748\n",
      "Epoch 148/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3821 - acc: 0.8345 - val_loss: 0.5355 - val_acc: 0.7789\n",
      "Epoch 149/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.4105 - acc: 0.8334 - val_loss: 0.5312 - val_acc: 0.7851\n",
      "Epoch 150/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3897 - acc: 0.8370 - val_loss: 0.5400 - val_acc: 0.7851\n",
      "Epoch 151/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3988 - acc: 0.8282 - val_loss: 0.5292 - val_acc: 0.7872\n",
      "Epoch 152/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.4081 - acc: 0.8288 - val_loss: 0.5201 - val_acc: 0.7893\n",
      "Epoch 153/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3825 - acc: 0.8474 - val_loss: 0.5251 - val_acc: 0.7872\n",
      "Epoch 154/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.4038 - acc: 0.8293 - val_loss: 0.5243 - val_acc: 0.7851\n",
      "Epoch 155/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3855 - acc: 0.8422 - val_loss: 0.5282 - val_acc: 0.7831\n",
      "Epoch 156/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3828 - acc: 0.8391 - val_loss: 0.5276 - val_acc: 0.7831\n",
      "Epoch 157/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3842 - acc: 0.8267 - val_loss: 0.5327 - val_acc: 0.7727\n",
      "Epoch 158/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3846 - acc: 0.8329 - val_loss: 0.5435 - val_acc: 0.7769\n",
      "Epoch 159/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3895 - acc: 0.8339 - val_loss: 0.5252 - val_acc: 0.7769\n",
      "Epoch 160/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3822 - acc: 0.8339 - val_loss: 0.5300 - val_acc: 0.7913\n",
      "Epoch 161/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.4165 - acc: 0.8314 - val_loss: 0.5138 - val_acc: 0.7893\n",
      "Epoch 162/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3962 - acc: 0.8298 - val_loss: 0.5149 - val_acc: 0.7789\n",
      "Epoch 163/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.3752 - acc: 0.8443 - val_loss: 0.5448 - val_acc: 0.7851\n",
      "Epoch 164/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.3924 - acc: 0.8365 - val_loss: 0.5292 - val_acc: 0.7872\n",
      "Epoch 165/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3936 - acc: 0.8329 - val_loss: 0.5241 - val_acc: 0.7893\n",
      "Epoch 166/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3871 - acc: 0.8355 - val_loss: 0.5309 - val_acc: 0.7851\n",
      "Epoch 167/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.4060 - acc: 0.8308 - val_loss: 0.5368 - val_acc: 0.7748\n",
      "Epoch 168/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3804 - acc: 0.8474 - val_loss: 0.5436 - val_acc: 0.7789\n",
      "Epoch 169/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.3857 - acc: 0.8495 - val_loss: 0.5314 - val_acc: 0.7727\n",
      "Epoch 170/500\n",
      "1933/1933 [==============================] - 0s 254us/step - loss: 0.3703 - acc: 0.8453 - val_loss: 0.5392 - val_acc: 0.7810\n",
      "Epoch 171/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3826 - acc: 0.8396 - val_loss: 0.5504 - val_acc: 0.7851\n",
      "Epoch 172/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3996 - acc: 0.8231 - val_loss: 0.5338 - val_acc: 0.7893\n",
      "Epoch 173/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3911 - acc: 0.8303 - val_loss: 0.5315 - val_acc: 0.7913\n",
      "Epoch 174/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3822 - acc: 0.8370 - val_loss: 0.5323 - val_acc: 0.7831\n",
      "Epoch 175/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3878 - acc: 0.8376 - val_loss: 0.5324 - val_acc: 0.7893\n",
      "Epoch 176/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3880 - acc: 0.8427 - val_loss: 0.5321 - val_acc: 0.7872\n",
      "Epoch 177/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3745 - acc: 0.8464 - val_loss: 0.5373 - val_acc: 0.7769\n",
      "Epoch 178/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3929 - acc: 0.8293 - val_loss: 0.5298 - val_acc: 0.7810\n",
      "Epoch 179/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3846 - acc: 0.8365 - val_loss: 0.5259 - val_acc: 0.7851\n",
      "Epoch 180/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3814 - acc: 0.8391 - val_loss: 0.5387 - val_acc: 0.7851\n",
      "Epoch 181/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3916 - acc: 0.8298 - val_loss: 0.5292 - val_acc: 0.7769\n",
      "Epoch 182/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3800 - acc: 0.8381 - val_loss: 0.5538 - val_acc: 0.7831\n",
      "Epoch 183/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.4016 - acc: 0.8365 - val_loss: 0.5277 - val_acc: 0.7831\n",
      "Epoch 184/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3892 - acc: 0.8391 - val_loss: 0.5382 - val_acc: 0.7810\n",
      "Epoch 185/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3901 - acc: 0.8329 - val_loss: 0.5215 - val_acc: 0.7831\n",
      "Epoch 186/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.4043 - acc: 0.8282 - val_loss: 0.5321 - val_acc: 0.7872\n",
      "Epoch 187/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3638 - acc: 0.8448 - val_loss: 0.5475 - val_acc: 0.7955\n",
      "Epoch 188/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3923 - acc: 0.8407 - val_loss: 0.5279 - val_acc: 0.7810\n",
      "Epoch 189/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3612 - acc: 0.8634 - val_loss: 0.5417 - val_acc: 0.7872\n",
      "Epoch 190/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3702 - acc: 0.8495 - val_loss: 0.5666 - val_acc: 0.7851\n",
      "Epoch 191/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3904 - acc: 0.8360 - val_loss: 0.5357 - val_acc: 0.7893\n",
      "Epoch 192/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3785 - acc: 0.8448 - val_loss: 0.5443 - val_acc: 0.7893\n",
      "Epoch 193/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3824 - acc: 0.8443 - val_loss: 0.5496 - val_acc: 0.7872\n",
      "Epoch 194/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3631 - acc: 0.8474 - val_loss: 0.5463 - val_acc: 0.7748\n",
      "Epoch 195/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3772 - acc: 0.8510 - val_loss: 0.5430 - val_acc: 0.7748\n",
      "Epoch 196/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3749 - acc: 0.8520 - val_loss: 0.5452 - val_acc: 0.7748\n",
      "Epoch 197/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3738 - acc: 0.8381 - val_loss: 0.5452 - val_acc: 0.7769\n",
      "Epoch 198/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3716 - acc: 0.8381 - val_loss: 0.5658 - val_acc: 0.7707\n",
      "Epoch 199/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3942 - acc: 0.8345 - val_loss: 0.5468 - val_acc: 0.7769\n",
      "Epoch 200/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3844 - acc: 0.8453 - val_loss: 0.5310 - val_acc: 0.7810\n",
      "Epoch 201/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3770 - acc: 0.8464 - val_loss: 0.5356 - val_acc: 0.7769\n",
      "Epoch 202/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3727 - acc: 0.8458 - val_loss: 0.5540 - val_acc: 0.7831\n",
      "Epoch 203/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3798 - acc: 0.8401 - val_loss: 0.5428 - val_acc: 0.7686\n",
      "Epoch 204/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3752 - acc: 0.8432 - val_loss: 0.5540 - val_acc: 0.7707\n",
      "Epoch 205/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3847 - acc: 0.8376 - val_loss: 0.5558 - val_acc: 0.7748\n",
      "Epoch 206/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3818 - acc: 0.8386 - val_loss: 0.5384 - val_acc: 0.7707\n",
      "Epoch 207/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3934 - acc: 0.8339 - val_loss: 0.5464 - val_acc: 0.7686\n",
      "Epoch 208/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3785 - acc: 0.8500 - val_loss: 0.5516 - val_acc: 0.7727\n",
      "Epoch 209/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.3697 - acc: 0.8557 - val_loss: 0.5496 - val_acc: 0.7748\n",
      "Epoch 210/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3822 - acc: 0.8412 - val_loss: 0.5467 - val_acc: 0.7665\n",
      "Epoch 211/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3711 - acc: 0.8510 - val_loss: 0.5661 - val_acc: 0.7707\n",
      "Epoch 212/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3798 - acc: 0.8427 - val_loss: 0.5490 - val_acc: 0.7769\n",
      "Epoch 213/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3713 - acc: 0.8546 - val_loss: 0.5586 - val_acc: 0.7769\n",
      "Epoch 214/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3709 - acc: 0.8407 - val_loss: 0.5620 - val_acc: 0.7789\n",
      "Epoch 215/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3841 - acc: 0.8464 - val_loss: 0.5516 - val_acc: 0.7707\n",
      "Epoch 216/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3730 - acc: 0.8422 - val_loss: 0.5789 - val_acc: 0.7769\n",
      "Epoch 217/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.3740 - acc: 0.8438 - val_loss: 0.5591 - val_acc: 0.7789\n",
      "Epoch 218/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3694 - acc: 0.8541 - val_loss: 0.5437 - val_acc: 0.7603\n",
      "Epoch 219/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3768 - acc: 0.8391 - val_loss: 0.5429 - val_acc: 0.7727\n",
      "Epoch 220/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.3838 - acc: 0.8376 - val_loss: 0.5355 - val_acc: 0.7645\n",
      "Epoch 221/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3636 - acc: 0.8432 - val_loss: 0.5464 - val_acc: 0.7727\n",
      "Epoch 222/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3646 - acc: 0.8469 - val_loss: 0.5653 - val_acc: 0.7686\n",
      "Epoch 223/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3912 - acc: 0.8339 - val_loss: 0.5483 - val_acc: 0.7789\n",
      "Epoch 224/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3865 - acc: 0.8391 - val_loss: 0.5408 - val_acc: 0.7748\n",
      "Epoch 225/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3586 - acc: 0.8562 - val_loss: 0.5596 - val_acc: 0.7789\n",
      "Epoch 226/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3455 - acc: 0.8655 - val_loss: 0.5571 - val_acc: 0.7769\n",
      "Epoch 227/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3722 - acc: 0.8412 - val_loss: 0.5548 - val_acc: 0.7810\n",
      "Epoch 228/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3774 - acc: 0.8350 - val_loss: 0.5419 - val_acc: 0.7727\n",
      "Epoch 229/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3855 - acc: 0.8407 - val_loss: 0.5222 - val_acc: 0.7789\n",
      "Epoch 230/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3732 - acc: 0.8391 - val_loss: 0.5543 - val_acc: 0.7727\n",
      "Epoch 231/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3782 - acc: 0.8474 - val_loss: 0.5329 - val_acc: 0.7810\n",
      "Epoch 232/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3712 - acc: 0.8443 - val_loss: 0.5383 - val_acc: 0.7831\n",
      "Epoch 233/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3694 - acc: 0.8412 - val_loss: 0.5603 - val_acc: 0.7810\n",
      "Epoch 234/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3828 - acc: 0.8443 - val_loss: 0.5464 - val_acc: 0.7872\n",
      "Epoch 235/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3861 - acc: 0.8438 - val_loss: 0.5421 - val_acc: 0.7934\n",
      "Epoch 236/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3755 - acc: 0.8515 - val_loss: 0.5416 - val_acc: 0.7831\n",
      "Epoch 237/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3986 - acc: 0.8365 - val_loss: 0.5357 - val_acc: 0.7769\n",
      "Epoch 238/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3594 - acc: 0.8557 - val_loss: 0.5471 - val_acc: 0.7789\n",
      "Epoch 239/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.3775 - acc: 0.8541 - val_loss: 0.5387 - val_acc: 0.7831\n",
      "Epoch 240/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3642 - acc: 0.8500 - val_loss: 0.5421 - val_acc: 0.7789\n",
      "Epoch 241/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.3641 - acc: 0.8386 - val_loss: 0.5376 - val_acc: 0.7727\n",
      "Epoch 242/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.3778 - acc: 0.8391 - val_loss: 0.5286 - val_acc: 0.7769\n",
      "Epoch 243/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3617 - acc: 0.8474 - val_loss: 0.5349 - val_acc: 0.7748\n",
      "Epoch 244/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3635 - acc: 0.8551 - val_loss: 0.5299 - val_acc: 0.7913\n",
      "Epoch 245/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3629 - acc: 0.8520 - val_loss: 0.5258 - val_acc: 0.7851\n",
      "Epoch 246/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3780 - acc: 0.8489 - val_loss: 0.5283 - val_acc: 0.7810\n",
      "Epoch 247/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.3754 - acc: 0.8515 - val_loss: 0.5130 - val_acc: 0.7872\n",
      "Epoch 248/500\n",
      "1933/1933 [==============================] - 1s 273us/step - loss: 0.3755 - acc: 0.8479 - val_loss: 0.5264 - val_acc: 0.7872\n",
      "Epoch 249/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3606 - acc: 0.8551 - val_loss: 0.5392 - val_acc: 0.7893\n",
      "Epoch 250/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3599 - acc: 0.8551 - val_loss: 0.5468 - val_acc: 0.7872\n",
      "Epoch 251/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3902 - acc: 0.8381 - val_loss: 0.5324 - val_acc: 0.7727\n",
      "Epoch 252/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3517 - acc: 0.8551 - val_loss: 0.5455 - val_acc: 0.7831\n",
      "Epoch 253/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3760 - acc: 0.8427 - val_loss: 0.5274 - val_acc: 0.7789\n",
      "Epoch 254/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3680 - acc: 0.8484 - val_loss: 0.5242 - val_acc: 0.7872\n",
      "Epoch 255/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3605 - acc: 0.8505 - val_loss: 0.5366 - val_acc: 0.7955\n",
      "Epoch 256/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3543 - acc: 0.8557 - val_loss: 0.5380 - val_acc: 0.7955\n",
      "Epoch 257/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3645 - acc: 0.8562 - val_loss: 0.5375 - val_acc: 0.7913\n",
      "Epoch 258/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3443 - acc: 0.8639 - val_loss: 0.5488 - val_acc: 0.7851\n",
      "Epoch 259/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3568 - acc: 0.8551 - val_loss: 0.5486 - val_acc: 0.7789\n",
      "Epoch 260/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3584 - acc: 0.8531 - val_loss: 0.5474 - val_acc: 0.7748\n",
      "Epoch 261/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3929 - acc: 0.8329 - val_loss: 0.5317 - val_acc: 0.7748\n",
      "Epoch 262/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3680 - acc: 0.8520 - val_loss: 0.5204 - val_acc: 0.7727\n",
      "Epoch 263/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3531 - acc: 0.8557 - val_loss: 0.5436 - val_acc: 0.7831\n",
      "Epoch 264/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3894 - acc: 0.8391 - val_loss: 0.5352 - val_acc: 0.7831\n",
      "Epoch 265/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3675 - acc: 0.8479 - val_loss: 0.5446 - val_acc: 0.7810\n",
      "Epoch 266/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.3681 - acc: 0.8520 - val_loss: 0.5429 - val_acc: 0.7831\n",
      "Epoch 267/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3654 - acc: 0.8583 - val_loss: 0.5446 - val_acc: 0.7769\n",
      "Epoch 268/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3600 - acc: 0.8588 - val_loss: 0.5375 - val_acc: 0.7769\n",
      "Epoch 269/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3697 - acc: 0.8432 - val_loss: 0.5440 - val_acc: 0.7789\n",
      "Epoch 270/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3620 - acc: 0.8598 - val_loss: 0.5656 - val_acc: 0.7769\n",
      "Epoch 271/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3616 - acc: 0.8546 - val_loss: 0.5547 - val_acc: 0.7727\n",
      "Epoch 272/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3735 - acc: 0.8526 - val_loss: 0.5329 - val_acc: 0.7831\n",
      "Epoch 273/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3683 - acc: 0.8469 - val_loss: 0.5291 - val_acc: 0.7769\n",
      "Epoch 274/500\n",
      "1933/1933 [==============================] - 0s 251us/step - loss: 0.3869 - acc: 0.8438 - val_loss: 0.5310 - val_acc: 0.7810\n",
      "Epoch 275/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3719 - acc: 0.8401 - val_loss: 0.5513 - val_acc: 0.7727\n",
      "Epoch 276/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3512 - acc: 0.8608 - val_loss: 0.5523 - val_acc: 0.7769\n",
      "Epoch 277/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3882 - acc: 0.8355 - val_loss: 0.5364 - val_acc: 0.7748\n",
      "Epoch 278/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3636 - acc: 0.8515 - val_loss: 0.5553 - val_acc: 0.7851\n",
      "Epoch 279/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3538 - acc: 0.8629 - val_loss: 0.5460 - val_acc: 0.7748\n",
      "Epoch 280/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3422 - acc: 0.8650 - val_loss: 0.5655 - val_acc: 0.7727\n",
      "Epoch 281/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3483 - acc: 0.8520 - val_loss: 0.5790 - val_acc: 0.7727\n",
      "Epoch 282/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3644 - acc: 0.8510 - val_loss: 0.5467 - val_acc: 0.7748\n",
      "Epoch 283/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3628 - acc: 0.8551 - val_loss: 0.5550 - val_acc: 0.7831\n",
      "Epoch 284/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3472 - acc: 0.8629 - val_loss: 0.5550 - val_acc: 0.7851\n",
      "Epoch 285/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3741 - acc: 0.8489 - val_loss: 0.5384 - val_acc: 0.7851\n",
      "Epoch 286/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3555 - acc: 0.8608 - val_loss: 0.5416 - val_acc: 0.7893\n",
      "Epoch 287/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3685 - acc: 0.8412 - val_loss: 0.5303 - val_acc: 0.7810\n",
      "Epoch 288/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3479 - acc: 0.8645 - val_loss: 0.5559 - val_acc: 0.7769\n",
      "Epoch 289/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3741 - acc: 0.8515 - val_loss: 0.5426 - val_acc: 0.7831\n",
      "Epoch 290/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3492 - acc: 0.8536 - val_loss: 0.5523 - val_acc: 0.7893\n",
      "Epoch 291/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3588 - acc: 0.8593 - val_loss: 0.5371 - val_acc: 0.7831\n",
      "Epoch 292/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3634 - acc: 0.8520 - val_loss: 0.5435 - val_acc: 0.7872\n",
      "Epoch 293/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3417 - acc: 0.8614 - val_loss: 0.5429 - val_acc: 0.7872\n",
      "Epoch 294/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.3640 - acc: 0.8541 - val_loss: 0.5409 - val_acc: 0.7831\n",
      "Epoch 295/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3733 - acc: 0.8422 - val_loss: 0.5305 - val_acc: 0.7851\n",
      "Epoch 296/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3695 - acc: 0.8495 - val_loss: 0.5385 - val_acc: 0.7831\n",
      "Epoch 297/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3530 - acc: 0.8427 - val_loss: 0.5480 - val_acc: 0.7851\n",
      "Epoch 298/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.3768 - acc: 0.8515 - val_loss: 0.5356 - val_acc: 0.7810\n",
      "Epoch 299/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3570 - acc: 0.8603 - val_loss: 0.5429 - val_acc: 0.7851\n",
      "Epoch 300/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3648 - acc: 0.8510 - val_loss: 0.5370 - val_acc: 0.7810\n",
      "Epoch 301/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3641 - acc: 0.8443 - val_loss: 0.5425 - val_acc: 0.7707\n",
      "Epoch 302/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3645 - acc: 0.8479 - val_loss: 0.5476 - val_acc: 0.7748\n",
      "Epoch 303/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3484 - acc: 0.8583 - val_loss: 0.5500 - val_acc: 0.7769\n",
      "Epoch 304/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3510 - acc: 0.8557 - val_loss: 0.5530 - val_acc: 0.7934\n",
      "Epoch 305/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3515 - acc: 0.8453 - val_loss: 0.5506 - val_acc: 0.7975\n",
      "Epoch 306/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3651 - acc: 0.8443 - val_loss: 0.5414 - val_acc: 0.7975\n",
      "Epoch 307/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3496 - acc: 0.8577 - val_loss: 0.5540 - val_acc: 0.7893\n",
      "Epoch 308/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3550 - acc: 0.8469 - val_loss: 0.5463 - val_acc: 0.7789\n",
      "Epoch 309/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3541 - acc: 0.8557 - val_loss: 0.5384 - val_acc: 0.7831\n",
      "Epoch 310/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3697 - acc: 0.8515 - val_loss: 0.5297 - val_acc: 0.7831\n",
      "Epoch 311/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3550 - acc: 0.8598 - val_loss: 0.5358 - val_acc: 0.7789\n",
      "Epoch 312/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3529 - acc: 0.8583 - val_loss: 0.5448 - val_acc: 0.7831\n",
      "Epoch 313/500\n",
      "1933/1933 [==============================] - 1s 270us/step - loss: 0.3668 - acc: 0.8469 - val_loss: 0.5341 - val_acc: 0.7789\n",
      "Epoch 314/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3578 - acc: 0.8572 - val_loss: 0.5386 - val_acc: 0.7789\n",
      "Epoch 315/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3727 - acc: 0.8401 - val_loss: 0.5320 - val_acc: 0.7789\n",
      "Epoch 316/500\n",
      "1933/1933 [==============================] - 1s 269us/step - loss: 0.3688 - acc: 0.8531 - val_loss: 0.5481 - val_acc: 0.7769\n",
      "Epoch 317/500\n",
      "1933/1933 [==============================] - 1s 276us/step - loss: 0.3767 - acc: 0.8443 - val_loss: 0.5393 - val_acc: 0.7934\n",
      "Epoch 318/500\n",
      "1933/1933 [==============================] - 1s 272us/step - loss: 0.3480 - acc: 0.8583 - val_loss: 0.5497 - val_acc: 0.7913\n",
      "Epoch 319/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3605 - acc: 0.8546 - val_loss: 0.5325 - val_acc: 0.7913\n",
      "Epoch 320/500\n",
      "1933/1933 [==============================] - 1s 267us/step - loss: 0.3594 - acc: 0.8505 - val_loss: 0.5307 - val_acc: 0.7934\n",
      "Epoch 321/500\n",
      "1933/1933 [==============================] - 1s 274us/step - loss: 0.3628 - acc: 0.8541 - val_loss: 0.5416 - val_acc: 0.7893\n",
      "Epoch 322/500\n",
      "1933/1933 [==============================] - 1s 268us/step - loss: 0.3406 - acc: 0.8655 - val_loss: 0.5316 - val_acc: 0.7872\n",
      "Epoch 323/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3398 - acc: 0.8696 - val_loss: 0.5378 - val_acc: 0.7831\n",
      "Epoch 324/500\n",
      "1933/1933 [==============================] - 1s 271us/step - loss: 0.3395 - acc: 0.8551 - val_loss: 0.5564 - val_acc: 0.7810\n",
      "Epoch 325/500\n",
      "1933/1933 [==============================] - 1s 266us/step - loss: 0.3636 - acc: 0.8608 - val_loss: 0.5383 - val_acc: 0.7893\n",
      "Epoch 326/500\n",
      "1933/1933 [==============================] - 1s 266us/step - loss: 0.3527 - acc: 0.8515 - val_loss: 0.5395 - val_acc: 0.7872\n",
      "Epoch 327/500\n",
      "1933/1933 [==============================] - 1s 268us/step - loss: 0.3586 - acc: 0.8577 - val_loss: 0.5481 - val_acc: 0.7810\n",
      "Epoch 328/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3506 - acc: 0.8500 - val_loss: 0.5436 - val_acc: 0.7831\n",
      "Epoch 329/500\n",
      "1933/1933 [==============================] - 1s 268us/step - loss: 0.3550 - acc: 0.8603 - val_loss: 0.5299 - val_acc: 0.7913\n",
      "Epoch 330/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3563 - acc: 0.8588 - val_loss: 0.5324 - val_acc: 0.7893\n",
      "Epoch 331/500\n",
      "1933/1933 [==============================] - 1s 274us/step - loss: 0.3459 - acc: 0.8500 - val_loss: 0.5398 - val_acc: 0.7893\n",
      "Epoch 332/500\n",
      "1933/1933 [==============================] - 1s 267us/step - loss: 0.3610 - acc: 0.8479 - val_loss: 0.5327 - val_acc: 0.7913\n",
      "Epoch 333/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3284 - acc: 0.8748 - val_loss: 0.5389 - val_acc: 0.7934\n",
      "Epoch 334/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3564 - acc: 0.8531 - val_loss: 0.5384 - val_acc: 0.7893\n",
      "Epoch 335/500\n",
      "1933/1933 [==============================] - 1s 267us/step - loss: 0.3517 - acc: 0.8598 - val_loss: 0.5334 - val_acc: 0.7851\n",
      "Epoch 336/500\n",
      "1933/1933 [==============================] - 1s 273us/step - loss: 0.3403 - acc: 0.8624 - val_loss: 0.5322 - val_acc: 0.7913\n",
      "Epoch 337/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3725 - acc: 0.8417 - val_loss: 0.5259 - val_acc: 0.7872\n",
      "Epoch 338/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3615 - acc: 0.8541 - val_loss: 0.5233 - val_acc: 0.7851\n",
      "Epoch 339/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3649 - acc: 0.8515 - val_loss: 0.5445 - val_acc: 0.7831\n",
      "Epoch 340/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3468 - acc: 0.8557 - val_loss: 0.5501 - val_acc: 0.7769\n",
      "Epoch 341/500\n",
      "1933/1933 [==============================] - 1s 259us/step - loss: 0.3615 - acc: 0.8458 - val_loss: 0.5399 - val_acc: 0.7810\n",
      "Epoch 342/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3587 - acc: 0.8577 - val_loss: 0.5350 - val_acc: 0.7789\n",
      "Epoch 343/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3597 - acc: 0.8572 - val_loss: 0.5463 - val_acc: 0.7748\n",
      "Epoch 344/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3592 - acc: 0.8526 - val_loss: 0.5377 - val_acc: 0.7727\n",
      "Epoch 345/500\n",
      "1933/1933 [==============================] - 1s 268us/step - loss: 0.3614 - acc: 0.8551 - val_loss: 0.5457 - val_acc: 0.7665\n",
      "Epoch 346/500\n",
      "1933/1933 [==============================] - 1s 271us/step - loss: 0.3580 - acc: 0.8541 - val_loss: 0.5370 - val_acc: 0.7748\n",
      "Epoch 347/500\n",
      "1933/1933 [==============================] - 1s 280us/step - loss: 0.3371 - acc: 0.8702 - val_loss: 0.5461 - val_acc: 0.7810\n",
      "Epoch 348/500\n",
      "1933/1933 [==============================] - 1s 278us/step - loss: 0.3325 - acc: 0.8655 - val_loss: 0.5491 - val_acc: 0.7789\n",
      "Epoch 349/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3617 - acc: 0.8520 - val_loss: 0.5413 - val_acc: 0.7831\n",
      "Epoch 350/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3454 - acc: 0.8608 - val_loss: 0.5514 - val_acc: 0.7810\n",
      "Epoch 351/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3622 - acc: 0.8531 - val_loss: 0.5346 - val_acc: 0.7872\n",
      "Epoch 352/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3582 - acc: 0.8495 - val_loss: 0.5422 - val_acc: 0.7789\n",
      "Epoch 353/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3471 - acc: 0.8588 - val_loss: 0.5413 - val_acc: 0.7686\n",
      "Epoch 354/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3446 - acc: 0.8645 - val_loss: 0.5465 - val_acc: 0.7624\n",
      "Epoch 355/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3594 - acc: 0.8489 - val_loss: 0.5591 - val_acc: 0.7686\n",
      "Epoch 356/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.3450 - acc: 0.8614 - val_loss: 0.5752 - val_acc: 0.7810\n",
      "Epoch 357/500\n",
      "1933/1933 [==============================] - 1s 269us/step - loss: 0.3622 - acc: 0.8567 - val_loss: 0.5620 - val_acc: 0.7789\n",
      "Epoch 358/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3588 - acc: 0.8484 - val_loss: 0.5624 - val_acc: 0.7769\n",
      "Epoch 359/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3390 - acc: 0.8691 - val_loss: 0.5526 - val_acc: 0.7789\n",
      "Epoch 360/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.3778 - acc: 0.8515 - val_loss: 0.5304 - val_acc: 0.7748\n",
      "Epoch 361/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3431 - acc: 0.8583 - val_loss: 0.5401 - val_acc: 0.7810\n",
      "Epoch 362/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3637 - acc: 0.8531 - val_loss: 0.5273 - val_acc: 0.7789\n",
      "Epoch 363/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3542 - acc: 0.8510 - val_loss: 0.5522 - val_acc: 0.7727\n",
      "Epoch 364/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3533 - acc: 0.8505 - val_loss: 0.5496 - val_acc: 0.7893\n",
      "Epoch 365/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.3445 - acc: 0.8629 - val_loss: 0.5671 - val_acc: 0.7851\n",
      "Epoch 366/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.3782 - acc: 0.8474 - val_loss: 0.5357 - val_acc: 0.7810\n",
      "Epoch 367/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3724 - acc: 0.8484 - val_loss: 0.5283 - val_acc: 0.7810\n",
      "Epoch 368/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3562 - acc: 0.8469 - val_loss: 0.5364 - val_acc: 0.7810\n",
      "Epoch 369/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3624 - acc: 0.8562 - val_loss: 0.5327 - val_acc: 0.7789\n",
      "Epoch 370/500\n",
      "1933/1933 [==============================] - 1s 274us/step - loss: 0.3416 - acc: 0.8676 - val_loss: 0.5426 - val_acc: 0.7789\n",
      "Epoch 371/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3294 - acc: 0.8624 - val_loss: 0.5519 - val_acc: 0.7748\n",
      "Epoch 372/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3498 - acc: 0.8629 - val_loss: 0.5476 - val_acc: 0.7769\n",
      "Epoch 373/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3545 - acc: 0.8583 - val_loss: 0.5327 - val_acc: 0.7831\n",
      "Epoch 374/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3504 - acc: 0.8567 - val_loss: 0.5528 - val_acc: 0.7831\n",
      "Epoch 375/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3436 - acc: 0.8583 - val_loss: 0.5414 - val_acc: 0.7789\n",
      "Epoch 376/500\n",
      "1933/1933 [==============================] - 1s 270us/step - loss: 0.3550 - acc: 0.8577 - val_loss: 0.5458 - val_acc: 0.7748\n",
      "Epoch 377/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3435 - acc: 0.8567 - val_loss: 0.5463 - val_acc: 0.7810\n",
      "Epoch 378/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3441 - acc: 0.8660 - val_loss: 0.5295 - val_acc: 0.7872\n",
      "Epoch 379/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3487 - acc: 0.8567 - val_loss: 0.5216 - val_acc: 0.7831\n",
      "Epoch 380/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3448 - acc: 0.8608 - val_loss: 0.5329 - val_acc: 0.7831\n",
      "Epoch 381/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3576 - acc: 0.8531 - val_loss: 0.5331 - val_acc: 0.7851\n",
      "Epoch 382/500\n",
      "1933/1933 [==============================] - 1s 266us/step - loss: 0.3469 - acc: 0.8526 - val_loss: 0.5374 - val_acc: 0.7789\n",
      "Epoch 383/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.3564 - acc: 0.8546 - val_loss: 0.5316 - val_acc: 0.7769\n",
      "Epoch 384/500\n",
      "1933/1933 [==============================] - 1s 266us/step - loss: 0.3469 - acc: 0.8567 - val_loss: 0.5507 - val_acc: 0.7707\n",
      "Epoch 385/500\n",
      "1933/1933 [==============================] - 0s 259us/step - loss: 0.3415 - acc: 0.8639 - val_loss: 0.5442 - val_acc: 0.7727\n",
      "Epoch 386/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3367 - acc: 0.8727 - val_loss: 0.5319 - val_acc: 0.7769\n",
      "Epoch 387/500\n",
      "1933/1933 [==============================] - 1s 278us/step - loss: 0.3567 - acc: 0.8577 - val_loss: 0.5438 - val_acc: 0.7851\n",
      "Epoch 388/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3346 - acc: 0.8650 - val_loss: 0.5770 - val_acc: 0.7789\n",
      "Epoch 389/500\n",
      "1933/1933 [==============================] - 1s 270us/step - loss: 0.3558 - acc: 0.8614 - val_loss: 0.5441 - val_acc: 0.7769\n",
      "Epoch 390/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3492 - acc: 0.8572 - val_loss: 0.5453 - val_acc: 0.7686\n",
      "Epoch 391/500\n",
      "1933/1933 [==============================] - 1s 284us/step - loss: 0.3388 - acc: 0.8681 - val_loss: 0.5547 - val_acc: 0.7748\n",
      "Epoch 392/500\n",
      "1933/1933 [==============================] - 1s 266us/step - loss: 0.3201 - acc: 0.8748 - val_loss: 0.5688 - val_acc: 0.7810\n",
      "Epoch 393/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3403 - acc: 0.8598 - val_loss: 0.5561 - val_acc: 0.7707\n",
      "Epoch 394/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3359 - acc: 0.8619 - val_loss: 0.5607 - val_acc: 0.7748\n",
      "Epoch 395/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3693 - acc: 0.8448 - val_loss: 0.5509 - val_acc: 0.7831\n",
      "Epoch 396/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3452 - acc: 0.8572 - val_loss: 0.5508 - val_acc: 0.7789\n",
      "Epoch 397/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3614 - acc: 0.8510 - val_loss: 0.5407 - val_acc: 0.7769\n",
      "Epoch 398/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3728 - acc: 0.8453 - val_loss: 0.5272 - val_acc: 0.7810\n",
      "Epoch 399/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3702 - acc: 0.8495 - val_loss: 0.5260 - val_acc: 0.7831\n",
      "Epoch 400/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3434 - acc: 0.8593 - val_loss: 0.5591 - val_acc: 0.7851\n",
      "Epoch 401/500\n",
      "1933/1933 [==============================] - 0s 257us/step - loss: 0.3582 - acc: 0.8510 - val_loss: 0.5426 - val_acc: 0.7810\n",
      "Epoch 402/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3470 - acc: 0.8546 - val_loss: 0.5322 - val_acc: 0.7851\n",
      "Epoch 403/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3372 - acc: 0.8614 - val_loss: 0.5520 - val_acc: 0.7789\n",
      "Epoch 404/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.3511 - acc: 0.8603 - val_loss: 0.5482 - val_acc: 0.7810\n",
      "Epoch 405/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3567 - acc: 0.8577 - val_loss: 0.5270 - val_acc: 0.7851\n",
      "Epoch 406/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3436 - acc: 0.8572 - val_loss: 0.5340 - val_acc: 0.7810\n",
      "Epoch 407/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3477 - acc: 0.8588 - val_loss: 0.5314 - val_acc: 0.7748\n",
      "Epoch 408/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3456 - acc: 0.8583 - val_loss: 0.5371 - val_acc: 0.7831\n",
      "Epoch 409/500\n",
      "1933/1933 [==============================] - 1s 264us/step - loss: 0.3500 - acc: 0.8551 - val_loss: 0.5384 - val_acc: 0.7810\n",
      "Epoch 410/500\n",
      "1933/1933 [==============================] - 1s 259us/step - loss: 0.3469 - acc: 0.8707 - val_loss: 0.5476 - val_acc: 0.7810\n",
      "Epoch 411/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3589 - acc: 0.8443 - val_loss: 0.5320 - val_acc: 0.7727\n",
      "Epoch 412/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.3455 - acc: 0.8583 - val_loss: 0.5498 - val_acc: 0.7810\n",
      "Epoch 413/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.3459 - acc: 0.8598 - val_loss: 0.5626 - val_acc: 0.7769\n",
      "Epoch 414/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.3194 - acc: 0.8722 - val_loss: 0.5661 - val_acc: 0.7769\n",
      "Epoch 415/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3439 - acc: 0.8572 - val_loss: 0.5584 - val_acc: 0.7789\n",
      "Epoch 416/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.3459 - acc: 0.8603 - val_loss: 0.5629 - val_acc: 0.7789\n",
      "Epoch 417/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3718 - acc: 0.8484 - val_loss: 0.5493 - val_acc: 0.7769\n",
      "Epoch 418/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.3491 - acc: 0.8598 - val_loss: 0.5660 - val_acc: 0.7686\n",
      "Epoch 419/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3460 - acc: 0.8583 - val_loss: 0.5748 - val_acc: 0.7769\n",
      "Epoch 420/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3571 - acc: 0.8515 - val_loss: 0.5610 - val_acc: 0.7707\n",
      "Epoch 421/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3523 - acc: 0.8603 - val_loss: 0.5412 - val_acc: 0.7686\n",
      "Epoch 422/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3414 - acc: 0.8743 - val_loss: 0.5571 - val_acc: 0.7769\n",
      "Epoch 423/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3530 - acc: 0.8510 - val_loss: 0.5476 - val_acc: 0.7769\n",
      "Epoch 424/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3414 - acc: 0.8593 - val_loss: 0.5561 - val_acc: 0.7748\n",
      "Epoch 425/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3294 - acc: 0.8650 - val_loss: 0.5655 - val_acc: 0.7851\n",
      "Epoch 426/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3321 - acc: 0.8655 - val_loss: 0.5589 - val_acc: 0.7789\n",
      "Epoch 427/500\n",
      "1933/1933 [==============================] - 1s 259us/step - loss: 0.3532 - acc: 0.8536 - val_loss: 0.5429 - val_acc: 0.7789\n",
      "Epoch 428/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3291 - acc: 0.8614 - val_loss: 0.5649 - val_acc: 0.7851\n",
      "Epoch 429/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3407 - acc: 0.8634 - val_loss: 0.5444 - val_acc: 0.7810\n",
      "Epoch 430/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3456 - acc: 0.8598 - val_loss: 0.5438 - val_acc: 0.7851\n",
      "Epoch 431/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3148 - acc: 0.8743 - val_loss: 0.5778 - val_acc: 0.7893\n",
      "Epoch 432/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3410 - acc: 0.8676 - val_loss: 0.5503 - val_acc: 0.7975\n",
      "Epoch 433/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3440 - acc: 0.8655 - val_loss: 0.5571 - val_acc: 0.7955\n",
      "Epoch 434/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3474 - acc: 0.8634 - val_loss: 0.5450 - val_acc: 0.7955\n",
      "Epoch 435/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3477 - acc: 0.8588 - val_loss: 0.5316 - val_acc: 0.7893\n",
      "Epoch 436/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3517 - acc: 0.8583 - val_loss: 0.5323 - val_acc: 0.7913\n",
      "Epoch 437/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3404 - acc: 0.8717 - val_loss: 0.5361 - val_acc: 0.7851\n",
      "Epoch 438/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3456 - acc: 0.8567 - val_loss: 0.5502 - val_acc: 0.7810\n",
      "Epoch 439/500\n",
      "1933/1933 [==============================] - 1s 261us/step - loss: 0.3555 - acc: 0.8608 - val_loss: 0.5449 - val_acc: 0.7665\n",
      "Epoch 440/500\n",
      "1933/1933 [==============================] - 1s 259us/step - loss: 0.3483 - acc: 0.8629 - val_loss: 0.5439 - val_acc: 0.7686\n",
      "Epoch 441/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3290 - acc: 0.8712 - val_loss: 0.5407 - val_acc: 0.7769\n",
      "Epoch 442/500\n",
      "1933/1933 [==============================] - 0s 259us/step - loss: 0.3532 - acc: 0.8551 - val_loss: 0.5459 - val_acc: 0.7748\n",
      "Epoch 443/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3356 - acc: 0.8681 - val_loss: 0.5729 - val_acc: 0.7831\n",
      "Epoch 444/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3378 - acc: 0.8717 - val_loss: 0.5536 - val_acc: 0.7769\n",
      "Epoch 445/500\n",
      "1933/1933 [==============================] - 1s 263us/step - loss: 0.3542 - acc: 0.8634 - val_loss: 0.5347 - val_acc: 0.7769\n",
      "Epoch 446/500\n",
      "1933/1933 [==============================] - 1s 262us/step - loss: 0.3422 - acc: 0.8614 - val_loss: 0.5472 - val_acc: 0.7748\n",
      "Epoch 447/500\n",
      "1933/1933 [==============================] - 1s 273us/step - loss: 0.3356 - acc: 0.8598 - val_loss: 0.5526 - val_acc: 0.7831\n",
      "Epoch 448/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3417 - acc: 0.8681 - val_loss: 0.5431 - val_acc: 0.7831\n",
      "Epoch 449/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.3526 - acc: 0.8546 - val_loss: 0.5523 - val_acc: 0.7748\n",
      "Epoch 450/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3220 - acc: 0.8722 - val_loss: 0.5927 - val_acc: 0.7851\n",
      "Epoch 451/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3367 - acc: 0.8645 - val_loss: 0.5703 - val_acc: 0.7748\n",
      "Epoch 452/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3398 - acc: 0.8702 - val_loss: 0.5730 - val_acc: 0.7913\n",
      "Epoch 453/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3374 - acc: 0.8707 - val_loss: 0.5707 - val_acc: 0.7810\n",
      "Epoch 454/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3357 - acc: 0.8650 - val_loss: 0.5661 - val_acc: 0.7769\n",
      "Epoch 455/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3506 - acc: 0.8520 - val_loss: 0.5571 - val_acc: 0.7769\n",
      "Epoch 456/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3668 - acc: 0.8500 - val_loss: 0.5682 - val_acc: 0.7872\n",
      "Epoch 457/500\n",
      "1933/1933 [==============================] - 0s 243us/step - loss: 0.3609 - acc: 0.8583 - val_loss: 0.5684 - val_acc: 0.7769\n",
      "Epoch 458/500\n",
      "1933/1933 [==============================] - 0s 244us/step - loss: 0.3375 - acc: 0.8702 - val_loss: 0.5749 - val_acc: 0.7851\n",
      "Epoch 459/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3599 - acc: 0.8479 - val_loss: 0.5557 - val_acc: 0.7707\n",
      "Epoch 460/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.3569 - acc: 0.8500 - val_loss: 0.5547 - val_acc: 0.7769\n",
      "Epoch 461/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3626 - acc: 0.8448 - val_loss: 0.5489 - val_acc: 0.7748\n",
      "Epoch 462/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3365 - acc: 0.8598 - val_loss: 0.5797 - val_acc: 0.7727\n",
      "Epoch 463/500\n",
      "1933/1933 [==============================] - 0s 244us/step - loss: 0.3491 - acc: 0.8629 - val_loss: 0.5662 - val_acc: 0.7769\n",
      "Epoch 464/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3555 - acc: 0.8619 - val_loss: 0.5523 - val_acc: 0.7686\n",
      "Epoch 465/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3321 - acc: 0.8733 - val_loss: 0.5762 - val_acc: 0.7769\n",
      "Epoch 466/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3468 - acc: 0.8567 - val_loss: 0.5619 - val_acc: 0.7748\n",
      "Epoch 467/500\n",
      "1933/1933 [==============================] - 0s 244us/step - loss: 0.3367 - acc: 0.8603 - val_loss: 0.5728 - val_acc: 0.7727\n",
      "Epoch 468/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3361 - acc: 0.8676 - val_loss: 0.5589 - val_acc: 0.7810\n",
      "Epoch 469/500\n",
      "1933/1933 [==============================] - 0s 243us/step - loss: 0.3481 - acc: 0.8546 - val_loss: 0.5617 - val_acc: 0.7893\n",
      "Epoch 470/500\n",
      "1933/1933 [==============================] - 0s 243us/step - loss: 0.3259 - acc: 0.8733 - val_loss: 0.5668 - val_acc: 0.7893\n",
      "Epoch 471/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.3321 - acc: 0.8670 - val_loss: 0.5653 - val_acc: 0.7748\n",
      "Epoch 472/500\n",
      "1933/1933 [==============================] - 1s 269us/step - loss: 0.3288 - acc: 0.8743 - val_loss: 0.5875 - val_acc: 0.7789\n",
      "Epoch 473/500\n",
      "1933/1933 [==============================] - 0s 258us/step - loss: 0.3436 - acc: 0.8670 - val_loss: 0.5652 - val_acc: 0.7748\n",
      "Epoch 474/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3333 - acc: 0.8686 - val_loss: 0.5708 - val_acc: 0.7748\n",
      "Epoch 475/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.3364 - acc: 0.8691 - val_loss: 0.5686 - val_acc: 0.7789\n",
      "Epoch 476/500\n",
      "1933/1933 [==============================] - 0s 254us/step - loss: 0.3429 - acc: 0.8551 - val_loss: 0.5615 - val_acc: 0.7707\n",
      "Epoch 477/500\n",
      "1933/1933 [==============================] - 0s 248us/step - loss: 0.3363 - acc: 0.8676 - val_loss: 0.5678 - val_acc: 0.7748\n",
      "Epoch 478/500\n",
      "1933/1933 [==============================] - 1s 266us/step - loss: 0.3577 - acc: 0.8572 - val_loss: 0.5558 - val_acc: 0.7872\n",
      "Epoch 479/500\n",
      "1933/1933 [==============================] - 1s 267us/step - loss: 0.3196 - acc: 0.8665 - val_loss: 0.5716 - val_acc: 0.7831\n",
      "Epoch 480/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3384 - acc: 0.8655 - val_loss: 0.5724 - val_acc: 0.7769\n",
      "Epoch 481/500\n",
      "1933/1933 [==============================] - 1s 265us/step - loss: 0.3483 - acc: 0.8634 - val_loss: 0.5608 - val_acc: 0.7727\n",
      "Epoch 482/500\n",
      "1933/1933 [==============================] - 0s 253us/step - loss: 0.3587 - acc: 0.8567 - val_loss: 0.5645 - val_acc: 0.7748\n",
      "Epoch 483/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3516 - acc: 0.8551 - val_loss: 0.5758 - val_acc: 0.7789\n",
      "Epoch 484/500\n",
      "1933/1933 [==============================] - 0s 256us/step - loss: 0.3398 - acc: 0.8676 - val_loss: 0.5792 - val_acc: 0.7789\n",
      "Epoch 485/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3177 - acc: 0.8696 - val_loss: 0.5935 - val_acc: 0.7645\n",
      "Epoch 486/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3475 - acc: 0.8603 - val_loss: 0.5792 - val_acc: 0.7872\n",
      "Epoch 487/500\n",
      "1933/1933 [==============================] - 0s 244us/step - loss: 0.3446 - acc: 0.8670 - val_loss: 0.5722 - val_acc: 0.7831\n",
      "Epoch 488/500\n",
      "1933/1933 [==============================] - 0s 246us/step - loss: 0.3393 - acc: 0.8670 - val_loss: 0.5808 - val_acc: 0.7831\n",
      "Epoch 489/500\n",
      "1933/1933 [==============================] - 0s 244us/step - loss: 0.3507 - acc: 0.8567 - val_loss: 0.5747 - val_acc: 0.7748\n",
      "Epoch 490/500\n",
      "1933/1933 [==============================] - 0s 255us/step - loss: 0.3287 - acc: 0.8577 - val_loss: 0.5809 - val_acc: 0.7789\n",
      "Epoch 491/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.3563 - acc: 0.8593 - val_loss: 0.5593 - val_acc: 0.7851\n",
      "Epoch 492/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3477 - acc: 0.8598 - val_loss: 0.5574 - val_acc: 0.7851\n",
      "Epoch 493/500\n",
      "1933/1933 [==============================] - 0s 250us/step - loss: 0.3518 - acc: 0.8546 - val_loss: 0.5598 - val_acc: 0.7851\n",
      "Epoch 494/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3465 - acc: 0.8562 - val_loss: 0.5560 - val_acc: 0.7727\n",
      "Epoch 495/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3429 - acc: 0.8598 - val_loss: 0.5667 - val_acc: 0.7769\n",
      "Epoch 496/500\n",
      "1933/1933 [==============================] - 0s 247us/step - loss: 0.3451 - acc: 0.8624 - val_loss: 0.5642 - val_acc: 0.7686\n",
      "Epoch 497/500\n",
      "1933/1933 [==============================] - 0s 245us/step - loss: 0.3419 - acc: 0.8619 - val_loss: 0.5646 - val_acc: 0.7748\n",
      "Epoch 498/500\n",
      "1933/1933 [==============================] - 0s 252us/step - loss: 0.3381 - acc: 0.8670 - val_loss: 0.5618 - val_acc: 0.7727\n",
      "Epoch 499/500\n",
      "1933/1933 [==============================] - 0s 249us/step - loss: 0.3490 - acc: 0.8577 - val_loss: 0.5584 - val_acc: 0.7727\n",
      "Epoch 500/500\n",
      "1933/1933 [==============================] - 1s 260us/step - loss: 0.3266 - acc: 0.8702 - val_loss: 0.5887 - val_acc: 0.7727\n",
      "acc: 94.77%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,trainY,validation_data=(X_test,testY) ,batch_size=batchSize, epochs=numEpochs, verbose=1)\n",
    "score = model.evaluate(X_train,trainY, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 77.27%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test,testY, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "[[1311   14]\n",
      " [  73  535]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1325\n",
      "           1       0.97      0.88      0.92       608\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1933\n",
      "   macro avg       0.96      0.93      0.95      1933\n",
      "weighted avg       0.96      0.95      0.95      1933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predY = model.predict(X_train)\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "\n",
    "prediction = model.predict_classes(X_train, verbose=0)\n",
    "print(confusion_matrix(y_train, prediction))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_train, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1933, 103)\n"
     ]
    }
   ],
   "source": [
    "with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n",
    "    \n",
    "    '''\n",
    "    Need to reconstruct the graph in DeepExplain context, using the same weights.\n",
    "    1. Get the input tensor\n",
    "    2. Get embedding tensor\n",
    "    3. Target the output of the last dense layer (pre-softmax)\n",
    "    '''\n",
    "    \n",
    "    inputTensor = model.layers[0].input\n",
    "    fModel = Model(inputs=inputTensor, outputs = model.layers[-1].output)\n",
    "    targetTensor = fModel(inputTensor)\n",
    "    \n",
    "    # Sample Data for attribution\n",
    "#     sampleX = trainX[num:num_next]    \n",
    "#     ys = trainY[num:num_next]\n",
    "    \n",
    "    # Sample Data for attribution\n",
    "    sampleX = X_train\n",
    "    ys = trainY\n",
    "    relevances = de.explain('elrp', targetTensor * ys, inputTensor, sampleX)\n",
    "    print(relevances.shape)\n",
    "    \n",
    "    relFeatures =  ['Att1', 'Att2', 'Att3', 'Att4', 'Att5', 'Att6', 'Att7', 'Att8', 'Att9', \n",
    "                    'Att10', 'Att11', 'Att12', 'Att13', 'Att14', 'Att15', 'Att16', 'Att17', \n",
    "                    'Att18', 'Att19', 'Att20', 'Att21', 'Att22', 'Att23', 'Att24', 'Att25', \n",
    "                    'Att26', 'Att27', 'Att28', 'Att29', 'Att30', 'Att31', 'Att32', 'Att33', \n",
    "                    'Att34', 'Att35', 'Att36', 'Att37', 'Att38', 'Att39', 'Att40', 'Att41', \n",
    "                    'Att42', 'Att43', 'Att44', 'Att45', 'Att46', 'Att47', 'Att48', 'Att49', \n",
    "                    'Att50', 'Att51', 'Att52', 'Att53', 'Att54', 'Att55', 'Att56', 'Att57', \n",
    "                    'Att58', 'Att59', 'Att60', 'Att61', 'Att62', 'Att63', 'Att64', 'Att65', \n",
    "                    'Att66', 'Att67', 'Att68', 'Att69', 'Att70', 'Att71', 'Att72', 'Att73', \n",
    "                    'Att74', 'Att75', 'Att76', 'Att77', 'Att78', 'Att79', 'Att80', 'Att81', \n",
    "                    'Att82', 'Att83', 'Att84', 'Att85', 'Att86', 'Att87', 'Att88', 'Att89', \n",
    "                    'Att90', 'Att91', 'Att92', 'Att93', 'Att94', 'Att95', 'Att96', 'Att97', \n",
    "                    'Att98', 'Att99', 'Att100', 'Att101', 'Att102', 'Att103']\n",
    "    for i in range(len(relFeatures)):\n",
    "        word = str(relFeatures[i])\n",
    "        originalRelevance = \"{:8.2f}\".format(relevances[0][i])\n",
    "#         print (\"\\t\\t\\t\" + str(originalRelevance) + \"\\t\" + word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "relDataFrame = pd.DataFrame.from_records(relevances, columns=relFeatures)\n",
    "# relDataFrame.plot(y=['Att1', 'Att2', 'Att3', 'Att4', 'Att5', 'Att6', 'Att7', 'Att8', 'Att9', \n",
    "#                     'Att10', 'Att11', 'Att12', 'Att13', 'Att14', 'Att15', 'Att16', 'Att17', \n",
    "#                     'Att18', 'Att19', 'Att20', 'Att21', 'Att22', 'Att23', 'Att24', 'Att25', \n",
    "#                     'Att26', 'Att27', 'Att28', 'Att29', 'Att30', 'Att31', 'Att32', 'Att33', \n",
    "#                     'Att34', 'Att35', 'Att36', 'Att37', 'Att38', 'Att39', 'Att40', 'Att41', \n",
    "#                     'Att42', 'Att43', 'Att44', 'Att45', 'Att46', 'Att47', 'Att48', 'Att49', \n",
    "#                     'Att50', 'Att51', 'Att52', 'Att53', 'Att54', 'Att55', 'Att56', 'Att57', \n",
    "#                     'Att58', 'Att59', 'Att60', 'Att61', 'Att62', 'Att63', 'Att64', 'Att65', \n",
    "#                     'Att66', 'Att67', 'Att68', 'Att69', 'Att70', 'Att71', 'Att72', 'Att73', \n",
    "#                     'Att74', 'Att75', 'Att76', 'Att77', 'Att78', 'Att79', 'Att80', 'Att81', \n",
    "#                     'Att82', 'Att83', 'Att84', 'Att85', 'Att86', 'Att87', 'Att88', 'Att89', \n",
    "#                     'Att90', 'Att91', 'Att92', 'Att93', 'Att94', 'Att95', 'Att96', 'Att97', \n",
    "#                     'Att98', 'Att99', 'Att100', 'Att101', 'Att102', 'Att103'], kind='line', figsize=(18,10), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7b1b34f390>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "relDataFrame.mean().plot( y=['Att1', 'Att2', 'Att3', 'Att4', 'Att5', 'Att6', 'Att7', 'Att8', 'Att9', \n",
    "                    'Att10', 'Att11', 'Att12', 'Att13', 'Att14', 'Att15', 'Att16', 'Att17', \n",
    "                    'Att18', 'Att19', 'Att20', 'Att21', 'Att22', 'Att23', 'Att24', 'Att25', \n",
    "                    'Att26', 'Att27', 'Att28', 'Att29', 'Att30', 'Att31', 'Att32', 'Att33', \n",
    "                    'Att34', 'Att35', 'Att36', 'Att37', 'Att38', 'Att39', 'Att40', 'Att41', \n",
    "                    'Att42', 'Att43', 'Att44', 'Att45', 'Att46', 'Att47', 'Att48', 'Att49', \n",
    "                    'Att50', 'Att51', 'Att52', 'Att53', 'Att54', 'Att55', 'Att56', 'Att57', \n",
    "                    'Att58', 'Att59', 'Att60', 'Att61', 'Att62', 'Att63', 'Att64', 'Att65', \n",
    "                    'Att66', 'Att67', 'Att68', 'Att69', 'Att70', 'Att71', 'Att72', 'Att73', \n",
    "                    'Att74', 'Att75', 'Att76', 'Att77', 'Att78', 'Att79', 'Att80', 'Att81', \n",
    "                    'Att82', 'Att83', 'Att84', 'Att85', 'Att86', 'Att87', 'Att88', 'Att89', \n",
    "                    'Att90', 'Att91', 'Att92', 'Att93', 'Att94', 'Att95', 'Att96', 'Att97', \n",
    "                    'Att98', 'Att99', 'Att100', 'Att101', 'Att102', 'Att103'], kind='bar', figsize=(15,8), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
